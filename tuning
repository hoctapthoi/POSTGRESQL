====== tools =======
pgtune
pg_configurator (pgconfigurator.cybertec.at)
postgresqltuner.pl

===== parallel ======
Postgres hỗ trợ parallel ở bước gather, select và update với cùng 1 session. 
bước delete, insert 
    không được hỗ trợ song song trong cùng 1 session.
    hỗ trợ song song nhiều session nếu chúng thực hiện trên những primary key khác nhau

===== force parallel =========
SET parallel_setup_cost = 0; -- giảm điều kiện để parallel
SET parallel_tuple_cost = 0; -- giảm điều kiện để parallel
SET max_parallel_workers_per_gather = 2; -- tối đa 2 worker chạy song song khi gather
SET max_parallel_workers = 8; -- tối đa 8 câu lệnh được chạy đồng thời, câu lệnh thứ 9 phải chờ đến khi worker được giải phóng

===== find hot table ==========
SELECT 
    schemaname,
    relname AS table_name,
    seq_scan,
    idx_scan,
    n_tup_ins AS inserts,
    n_tup_upd AS updates,
    n_tup_del AS deletes,
    n_live_tup AS live_tuples,
    n_dead_tup AS dead_tuples
FROM 
    pg_stat_user_tables
ORDER BY 
    (n_tup_ins + n_tup_upd + n_tup_del) DESC
LIMIT 10;

===== pg_buffercache =========
extension verify buffer cache
# CREATE EXTENSION pg_buffercache
# \dx

===== pg_prewarm======
--- extension for loading table into buffer cache ===
# CREATE EXTENSION pg_prewarm 
# \dx
# SELECT pg_prewarm('t1');
--- verify t1 in or out of buffercache
SELECT c.relname, count(*) AS buffers
FROM pg_buffercache b
JOIN pg_class c ON b.relfilenode = c.relfilenode
WHERE c.relname = 't1'  -- Replace 't1' with your table name
GROUP BY c.relname;

============= configure memory component with ex: 64GB memory, 100 connection, 8 CPU ========
===== shared_buffers ======
data from disk loaded into shared buffer for caching. data in buffer run faster than from disk
25-40% memory = 25-40% x 64 = 16-25GB
# alter system set shared_buffers = '16GB'; -> need restart instance;

===== work_mem size ======
work_mem use for order/distinct/merge join/hash join. if work_mem not enough, query will use temp file
25% memory / connection number = 25% x 64GB / 100 = 16MB
if existing much soft -> increate work_mem size. ex: 24MB, 32MB ...
configure work_mem for level:
- session level: # set work_mem = '32MB';
- query level: # BEGIN;
                 SET LOCAL work_mem = '64MB';
                 -- Your specific query here
                 COMMIT;
- user/role level: # ALTER ROLE some_user SET work_mem = '16MB';
- check current work_mem size: show work_mem;
set log_temp_files = 0 in postgresql.conf to monitoring temp file
log message: LOG: temporary file: path "base/pgsql_tmp/...", size <size_in_bytes>

===== maintenance_work_mem ========
maintenance_work_mem use for create index, add FK, autovacuum
50MB for each system memory, limit 2GB = 0.05 x 64GB = 3.2GB (> 2GB) = 2GB

===== autovacuum_work_mem ========
default, auto vacuum use maintenance work mem for doing the job
system memory / 16 , limit 1GB = 64GB / 16 = 4GB (> 1GB) = 1GB

===== connection =====
authentication_timeout 
tcp_keepalive
